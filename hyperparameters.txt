LSTM Deep Learning

Hyperparameters for each LSTM Layer:
lstm_units:Dimensionality of the output space. values:(2,8,16,32,64,128)
lstm_activation:Activation function to use. values:(tanh, sigmoid, relu, linear, hard_sigmoid)
recurrent_activation:Activation function to use for the recurrent step. values:(tanh, sigmoid, relu, linear, hard_sigmoid)

The hyperparameters of each Dense Layer are:
number_neurons:Dimensionality of the output space values:(2,8,16,32,64,128)
activation:Activation function to use values:(tanh, sigmoid, relu, linear)
dropout:After each Dense Layer a dropout layer is added with possible dropout values of:(0.1,0.2,0.3,0.4,0.5)

Also the network’s hyperparameters include:
lookback: The number of backward steps that are being fed to the model as input. values:(1,2,3,4,5)
LSTM_Layers:Number of LSTM Layers the network has  values:(1,2)
Dense_Layers:Number of Dense Layers the network has values:(1,2,3,4,5)
Optimizer:The network’s training optimizer values:(rmsprop, adam, sgd, adagrad, adadelta, adamax, nadam)
Learning_rate: The learning rate of the model. values:(0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2)
batch_size: The size of training instances that the model trains for before reevaluating its weights. values:(1,10,50,100,500,1000)
epochs: The number of maximum epochs of training. The models are using early stopping to complete their training once they stop getting better. values:(20,50,100,200)


sklearn MultiOutputRegressor
This sklearn class helps us extend the regressors that do not support multi-output regression. It takes the input and uses it to train a separate regressor for each output. It is used on XGBoost, auto-sklearn and SVM methods.


XGBoost

learning_rate values:(0.05,0.15,0.3)

max_depth: values:(3,5,10,15)
Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.

min_child_weight: values:(3,7)
Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be.

gamma: values:(0,0.2,0.4)
Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.

colsample_bytree: values:(0.3,0.7)
colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.
 

SVM

kernel = Kernel type of the Regressor. values:(linear, poly, rbf, sigmoid)
degree = Degree of the polynomial kernel. values:(1, 2, 3, 4, 5, 6, 7)
gamma = Kernel coefficient used in polynomial, rbf and sigmoid. values:(scale, auto)
value 'scale' means gamma = 1 / (n_features * X.var()), and value 'auto' means gamma = 1/n_features
C = Regularization parameter. The strength of regularization is inversely proportional to this hyperparameter. values:(0.1, 0.5, 1.0, 1.5, 2.0, 3.0)
coef0 = Independent term in kernel function, used in poly and sigmoid kernels. values:( -2.0, -1.0, -0.5, -0.1, 0, 0.1, 0.5, 1.0, 2.0)

*οι περιγραφές των υπερπαραμέτρων για SVM και XGBoost είναι από τα documentation pages
